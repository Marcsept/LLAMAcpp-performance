[1719315906] Log start
[1719315906] Cmd: ../llama.cpp/main -m models/gpt2.gguf -s 987654321 --top-p 0.99 -t 1 -np 0.9 -ns 10 -ps 1 --file inputs/input1.txt -n 5
[1719315906] main: build = 3126 (582820c5)
[1719315906] main: built with cc (GCC) 14.1.0 for x86_64-w64-mingw32
[1719315906] main: seed  = 987654321
[1719315906] main: llama backend init
[1719315906] main: load the model and apply lora adapter, if any
[1719315906] llama_model_loader: loaded meta data with 16 key-value pairs and 293 tensors from models/gpt2.gguf (version GGUF V3 (latest))
[1719315906] llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
[1719315906] llama_model_loader: - kv   0:                       general.architecture str              = gpt2
[1719315906] llama_model_loader: - kv   1:                               general.name str              = gpt2-medium
[1719315906] llama_model_loader: - kv   2:                           gpt2.block_count u32              = 24
[1719315906] llama_model_loader: - kv   3:                        gpt2.context_length u32              = 1024
[1719315906] llama_model_loader: - kv   4:                      gpt2.embedding_length u32              = 1024
[1719315906] llama_model_loader: - kv   5:                   gpt2.feed_forward_length u32              = 4096
[1719315906] llama_model_loader: - kv   6:                  gpt2.attention.head_count u32              = 16
[1719315906] llama_model_loader: - kv   7:          gpt2.attention.layer_norm_epsilon f32              = 0.000010
[1719315906] llama_model_loader: - kv   8:                          general.file_type u32              = 25
[1719315906] llama_model_loader: - kv   9:                       tokenizer.ggml.model str              = gpt2
[1719315906] llama_model_loader: - kv  10:                      tokenizer.ggml.tokens arr[str,50257]   = ["!", "\"", "#", "$", "%", "&", "'", ...
[1719315906] llama_model_loader: - kv  11:                  tokenizer.ggml.token_type arr[i32,50257]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
[1719315906] llama_model_loader: - kv  12:                      tokenizer.ggml.merges arr[str,50000]   = ["Ġ t", "Ġ a", "h e", "i n", "r e",...
[1719315906] llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 50256
[1719315906] llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 50256
[1719315906] llama_model_loader: - kv  15:               general.quantization_version u32              = 2
[1719315906] llama_model_loader: - type  f32:  194 tensors
[1719315906] llama_model_loader: - type  f16:    1 tensors
[1719315906] llama_model_loader: - type q5_K:    3 tensors
[1719315906] llama_model_loader: - type q6_K:    1 tensors
[1719315906] llama_model_loader: - type iq4_nl:   94 tensors
[1719315906] llm_load_vocab: missing pre-tokenizer type, using: 'default'
[1719315906] llm_load_vocab:                                             
[1719315906] llm_load_vocab: ************************************        
[1719315906] llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        
[1719315906] llm_load_vocab: CONSIDER REGENERATING THE MODEL             
[1719315906] llm_load_vocab: ************************************        
[1719315906] llm_load_vocab:                                             
[1719315906] llm_load_vocab: special tokens cache size = 1
[1719315906] llm_load_vocab: token to piece cache size = 0.3060 MB
[1719315906] llm_load_print_meta: format           = GGUF V3 (latest)
[1719315906] llm_load_print_meta: arch             = gpt2
[1719315906] llm_load_print_meta: vocab type       = BPE
[1719315906] llm_load_print_meta: n_vocab          = 50257
[1719315906] llm_load_print_meta: n_merges         = 50000
[1719315906] llm_load_print_meta: n_ctx_train      = 1024
[1719315906] llm_load_print_meta: n_embd           = 1024
[1719315906] llm_load_print_meta: n_head           = 16
[1719315906] llm_load_print_meta: n_head_kv        = 16
[1719315906] llm_load_print_meta: n_layer          = 24
[1719315906] llm_load_print_meta: n_rot            = 64
[1719315906] llm_load_print_meta: n_embd_head_k    = 64
[1719315906] llm_load_print_meta: n_embd_head_v    = 64
[1719315906] llm_load_print_meta: n_gqa            = 1
[1719315906] llm_load_print_meta: n_embd_k_gqa     = 1024
[1719315906] llm_load_print_meta: n_embd_v_gqa     = 1024
[1719315906] llm_load_print_meta: f_norm_eps       = 1.0e-05
[1719315906] llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
[1719315906] llm_load_print_meta: f_clamp_kqv      = 0.0e+00
[1719315906] llm_load_print_meta: f_max_alibi_bias = 0.0e+00
[1719315906] llm_load_print_meta: f_logit_scale    = 0.0e+00
[1719315906] llm_load_print_meta: n_ff             = 4096
[1719315906] llm_load_print_meta: n_expert         = 0
[1719315906] llm_load_print_meta: n_expert_used    = 0
[1719315906] llm_load_print_meta: causal attn      = 1
[1719315906] llm_load_print_meta: pooling type     = 0
[1719315906] llm_load_print_meta: rope type        = -1
[1719315906] llm_load_print_meta: rope scaling     = linear
[1719315906] llm_load_print_meta: freq_base_train  = 10000.0
[1719315906] llm_load_print_meta: freq_scale_train = 1
[1719315906] llm_load_print_meta: n_ctx_orig_yarn  = 1024
[1719315906] llm_load_print_meta: rope_finetuned   = unknown
[1719315906] llm_load_print_meta: ssm_d_conv       = 0
[1719315906] llm_load_print_meta: ssm_d_inner      = 0
[1719315906] llm_load_print_meta: ssm_d_state      = 0
[1719315906] llm_load_print_meta: ssm_dt_rank      = 0
[1719315906] llm_load_print_meta: model type       = 0.4B
[1719315906] llm_load_print_meta: model ftype      = IQ4_NL - 4.5 bpw
[1719315906] llm_load_print_meta: model params     = 406.29 M
[1719315906] llm_load_print_meta: model size       = 234.59 MiB (4.84 BPW) 
[1719315906] llm_load_print_meta: general.name     = gpt2-medium
[1719315906] llm_load_print_meta: BOS token        = 50256 '<|endoftext|>'
[1719315906] llm_load_print_meta: EOS token        = 50256 '<|endoftext|>'
[1719315906] llm_load_print_meta: LF token         = 128 'Ä'
[1719315906] llm_load_print_meta: EOT token        = 50256 '<|endoftext|>'
[1719315906] llm_load_tensors: ggml ctx size =    0.14 MiB
[1719315906] llm_load_tensors:        CPU buffer size =   234.59 MiB
[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] .[1719315906] 
[1719315906] llama_new_context_with_model: n_ctx      = 1024
[1719315906] llama_new_context_with_model: n_batch    = 1024
[1719315906] llama_new_context_with_model: n_ubatch   = 512
[1719315906] llama_new_context_with_model: flash_attn = 0
[1719315906] llama_new_context_with_model: freq_base  = 10000.0
[1719315906] llama_new_context_with_model: freq_scale = 1
[1719315906] llama_kv_cache_init:        CPU KV buffer size =    96.00 MiB
[1719315906] llama_new_context_with_model: KV self size  =   96.00 MiB, K (f16):   48.00 MiB, V (f16):   48.00 MiB
[1719315906] llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
[1719315906] llama_new_context_with_model:        CPU compute buffer size =   100.16 MiB
[1719315906] llama_new_context_with_model: graph nodes  = 897
[1719315906] llama_new_context_with_model: graph splits = 1
[1719315906] warming up the model with an empty run
[1719315910] n_ctx: 1024
[1719315910] 
[1719315910] system_info: n_threads = 1 / 4 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
[1719315910] add_bos: 0
[1719315910] tokenize the prompt
[1719315910] prompt: "Imagine a world 50 years from now. What technological advancements do you think will define our lives, and how will they impact society?"
[1719315910] tokens: [ '<|endoftext|>':50256, 'Imagine':25153, ' a':257, ' world':995, ' ':220, '50':1120, ' years':812, ' from':422, ' now':783, '.':13, ' What':1867, ' technological':14614, ' advancements':47220, ' do':466, ' you':345, ' think':892, ' will':481, ' define':8160, ' our':674, ' lives':3160, ',':11, ' and':290, ' how':703, ' will':481, ' they':484, ' impact':2928, ' society':3592, '?':30 ]
[1719315910] recalculate the cached logits (check): embd_inp.empty() false, n_matching_session_tokens 0, embd_inp.size() 28, session_tokens.size() 0, embd_inp.size() 28
[1719315910] sampling: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	top_k = 40, tfs_z = 1.000, top_p = 0.990, min_p = 0.050, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
[1719315910] sampling order: 
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature 
[1719315910] generate: n_ctx = 1024, n_batch = 2048, n_predict = 5, n_keep = 0
[1719315910] 

[1719315910] embd_inp.size(): 28, n_consumed: 0
[1719315910] eval: [ '<|endoftext|>':50256, 'Imagine':25153, ' a':257, ' world':995, ' ':220, '50':1120, ' years':812, ' from':422, ' now':783, '.':13, ' What':1867, ' technological':14614, ' advancements':47220, ' do':466, ' you':345, ' think':892, ' will':481, ' define':8160, ' our':674, ' lives':3160, ',':11, ' and':290, ' how':703, ' will':481, ' they':484, ' impact':2928, ' society':3592, '?':30 ]
[1719315952] n_past = 28
[1719315952] last: [ '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '<|endoftext|>':50256, 'Imagine':25153, ' a':257, ' world':995, ' ':220, '50':1120, ' years':812, ' from':422, ' now':783, '.':13, ' What':1867, ' technological':14614, ' advancements':47220, ' do':466, ' you':345, ' think':892, ' will':481, ' define':8160, ' our':674, ' lives':3160, ',':11, ' and':290, ' how':703, ' will':481, ' they':484, ' impact':2928, ' society':3592, '?':30, '':198 ]
[1719315952] n_remain: 4
[1719315952] eval: [ '':198 ]
[1719315955] n_past = 29
[1719315955] last: [ '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '<|endoftext|>':50256, 'Imagine':25153, ' a':257, ' world':995, ' ':220, '50':1120, ' years':812, ' from':422, ' now':783, '.':13, ' What':1867, ' technological':14614, ' advancements':47220, ' do':466, ' you':345, ' think':892, ' will':481, ' define':8160, ' our':674, ' lives':3160, ',':11, ' and':290, ' how':703, ' will':481, ' they':484, ' impact':2928, ' society':3592, '?':30, '':198, '':198 ]
[1719315955] n_remain: 3
[1719315955] eval: [ '':198 ]
[1719315958] n_past = 30
[1719315958] last: [ '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '<|endoftext|>':50256, 'Imagine':25153, ' a':257, ' world':995, ' ':220, '50':1120, ' years':812, ' from':422, ' now':783, '.':13, ' What':1867, ' technological':14614, ' advancements':47220, ' do':466, ' you':345, ' think':892, ' will':481, ' define':8160, ' our':674, ' lives':3160, ',':11, ' and':290, ' how':703, ' will':481, ' they':484, ' impact':2928, ' society':3592, '?':30, '':198, '':198, 'Here':4342 ]
[1719315958] n_remain: 2
[1719315958] eval: [ 'Here':4342 ]
[1719315961] n_past = 31
[1719315961] last: [ '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '<|endoftext|>':50256, 'Imagine':25153, ' a':257, ' world':995, ' ':220, '50':1120, ' years':812, ' from':422, ' now':783, '.':13, ' What':1867, ' technological':14614, ' advancements':47220, ' do':466, ' you':345, ' think':892, ' will':481, ' define':8160, ' our':674, ' lives':3160, ',':11, ' and':290, ' how':703, ' will':481, ' they':484, ' impact':2928, ' society':3592, '?':30, '':198, '':198, 'Here':4342, ' are':389 ]
[1719315961] n_remain: 1
[1719315961] eval: [ ' are':389 ]
[1719315965] n_past = 32
[1719315965] last: [ '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '!':0, '<|endoftext|>':50256, 'Imagine':25153, ' a':257, ' world':995, ' ':220, '50':1120, ' years':812, ' from':422, ' now':783, '.':13, ' What':1867, ' technological':14614, ' advancements':47220, ' do':466, ' you':345, ' think':892, ' will':481, ' define':8160, ' our':674, ' lives':3160, ',':11, ' and':290, ' how':703, ' will':481, ' they':484, ' impact':2928, ' society':3592, '?':30, '':198, '':198, 'Here':4342, ' are':389, ' 10':838 ]
[1719315965] n_remain: 0
[1719315965] 
[1719315965] llama_print_timings:        load time =    4326.56 ms
[1719315965] llama_print_timings:      sample time =       0.39 ms /     5 runs   (    0.08 ms per token, 12755.10 tokens per second)
[1719315965] llama_print_timings: prompt eval time =   41683.24 ms /    28 tokens ( 1488.69 ms per token,     0.67 tokens per second)
[1719315965] llama_print_timings:        eval time =   12725.96 ms /     4 runs   ( 3181.49 ms per token,     0.31 tokens per second)
[1719315965] llama_print_timings:       total time =   54413.08 ms /    32 tokens
[1719315965] Log end
